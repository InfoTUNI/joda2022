{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Text to Group Amazon Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this demo session is to cluster a set of reviews scraped from the Amazon.com e-commerce platform. We will use the textual content of each review to extract features by implementing the TF-IDF technique. These features are used from the k-means clustering algorithm to group reviews with similar features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading dataset github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('out.json'):\n",
    "    df = pd.read_json('out.json')\n",
    "else:\n",
    "    df = pd.read_json('https://raw.githubusercontent.com/InfoTUNI/joda2022/master/koodiesimerkit/out.json')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we preprocess the textual content of each rating. There are many techniques we could use to clean the data but given the relatively short length of the average rating's content, we only consider converting the text field of each rating to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "Next, let's make sure that we convert the rating attribute to a floating point value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_rating = lambda rating: float(rating[:2])\n",
    "\n",
    "df['rating'] = df['rating'].apply(conv_rating)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "There are various methods to extract features out of textual data. Such techniques are known as word representation techniques. Here we list two popular and relatively simple approaches to the word representation problem:\n",
    "\n",
    "- Among the simplest ones is the Bag-of-Words(BoW) technique. Through this approach we can represent a text (such as a sentence or a document) is represented as the bag of its words, disregarding grammar and the word order. <br/> Ex: \"the quick brown fox jumps over the lazy dog\" ==> <br/>{'the':2, 'quick':1, 'brown':1, 'fox':1, 'jumps':1, 'over':1, 'lazy':1, 'dog':1}\n",
    "- TF-IDF stands for \"Term Frequency - Inverse Document Frequency\". This approach is build on top of the BoW technique. The count(frequency) of each word in a document is normalized by the number of times that each specific word appears over the set of all documents. In this way, popular words appearing among multiple documents will get lower tf-idf score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "# suppress errors when dividing by 0\n",
    "np.seterr(divide='ignore', invalid='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "reprs = vectorizer.fit_transform(df['text'])\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse arrays to dense ones and normalize the values of each vector in repr to be of unit length.\n",
    "# Our clustering algorithm uses euclidean-distance to group similar vectors together. By performing the following\n",
    "# normalization we get the equivalent effect as if we group them based on the cosine distance.\n",
    "reprs = reprs.toarray()\n",
    "length = np.sqrt((reprs**2).sum(axis=1))[:,None]\n",
    "reprs = reprs / length\n",
    "\n",
    "# replace Inf and Nan values\n",
    "reprs = np.nan_to_num(reprs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "print('The shape of the matrix containing the word representations is:', reprs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print example features\n",
    "reprs[0][:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering the ratings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means is an unsupervised clustering algorithm. Given a set of **n** vectors, k-means will group them into **k** groups/clusters while trying to keep the variance(distance) between vectors of the same group small. For our use-case, we will use the tf-idf generated representations as the input vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "k = 6\n",
    "kmeans = KMeans(n_clusters=k, random_state=11)\n",
    "clusters = kmeans.fit_predict(reprs)\n",
    "\n",
    "print(clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster_id'] = clusters\n",
    "\n",
    "df['cluster_id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_ratings = df.groupby('cluster_id').mean()\n",
    "\n",
    "fig = average_ratings.plot.bar(figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "Now we will plot each rating on a 2d scatter-plot. Currently, the length/dimensions of the representation vectors is more than 3000. We will use the PCA dimensionality reduction tool to reduce the number of dimensions to 2. In the [tutorials folder] (https://github.com/InfoTUNI/joda2022/tree/master/tutorials) you may find a more detailed explanation of the PCA technique. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Initialize PCA with 2 components, for 2d visualising.\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce representation vectors\n",
    "reducedX = pca.fit_transform(reprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "# generate k colors\n",
    "color_map = lambda cid: plt.cm.get_cmap('hsv', k)(cid)\n",
    "\n",
    "for dp, cid in zip(reducedX, clusters):\n",
    "    ax.scatter(dp[0], dp[1], s=50, color=color_map(cid))\n",
    "\n",
    "    \n",
    "plt.legend(handles=[\n",
    "                        mpatches.Patch(color=color_map(cid), label='Cluster '+str(cid)) for cid in range(k)\n",
    "        ])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['cluster_id']==5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Thank you')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
